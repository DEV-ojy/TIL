# 비지도 학습 기반 문장 표현

문장에 대한 분산표상도 단어 임베딩처럼 비지도방식으로 학습할 수 있습니다 이러한 비지도 학습의 결과는 임의의 문장을,의미와 문법적 속성이 내재한 고정 크기의 벡터에 매핑하는 `문장 인코더`입니다 

일반적으로 학습과정을 위해 보조적인 작업을 정의해야 합니다 

단어 임베딩을 위한 skip-gram 모델(Mikolov et al., 2013b)과 유사하게, 문장 임베딩을 위한 skip-thought 모델(Kiros et al., 2015)이 제안됐습니다  

skip-thought 모델에서 보조적인 과제는 주어진 문장 앞뒤에 있는 두 개의 인접 문장을 예측하는 것으로 여기에는 seq2seq 모델이 사용됩니다 
하나의 LSTM이 문장을 벡터(분산표상)로 인코딩해 두 개의 다른 LSTM은 이 벡터를 디코딩하여 타겟 시퀀스를 생성합니다 

학습을 마친 인코더는 문장의 피처(feature)를 뽑아내는 추출기로 볼 수 있습니다(단어 임베딩 또한 동시에 학습)

Kiros et al. (2015)는 문장 분류(sentence classification) 문제로 학습된 문장 인코더의 품질을 검증했습니다 

정적인 특징 벡터(static feature vector)[^5] 기반의 단순 선형 모델로 경쟁력 있는 결과를 보여주었다. 그러나 문장 인코더는 학습 과정에서 미세 조정(fine-tune)될 수 있습니다 

Dai and Le(2015)는 오토인코더(autoencoder, Rumelhart et al., 1985)와 유사하게, 인코딩된 문장 자체를 재구축(reconstruct)하는 디코더를 사용했습니다 

언어모델링(Language modeling)은 LSTM 인코더를 학습할 때 보조적인 작업으로 사용될 수도 있습니다 
감독 신호(supervision signal)는 다음 토큰의 예측에서 비롯하여 Dai and Le(2015)는 다양한 작업에서 사전 학습된 파라메터를 LSTM 모델의 초기값으로 쓰는 실험을 수행했습니다 
그들은 방대한 비지도 말뭉치로 사전 학습된 문장 인코더가 단어 임베딩만 사전학습해 사용하는 것보다 더 나은 정확성을 보임을 입증하여 다음 토큰을 예측하는 것은 문장 자체를 재구축(reconstruct)하는 것보다 더 나쁜 보조 목표인 것으로 판명합니다 LSTM의 히든 스테이트는 단기(shor-term) 기억에만 충실하기 때문입니다 
