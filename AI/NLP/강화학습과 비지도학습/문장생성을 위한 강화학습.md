# 문장생성을 위한 강화학습 

강화학습(Reinforcement Learning)은 보상(reward)을 얻기 전에 행동(action)을 수행하도록 에이전트(agent)를 학습시키는 기법이다

기존의 RNN기반 언어생성기는 일반적으로 현재 히든 스테이트와 이전토큰이 주어졌을때 정답 단어가 나타날 우도(likelihood)를 최대화함으로써 학습됩니다 

`Teacher forcing`이라 불리는 기법은 RNN학습 과정에서 이전 스템의 정답 단어들을 다음 스템의 입력값으로 대체됩니다 
그러나 추론 과정에서는 이전 토큰은 모델 자체에서 생성된 토큰으로 대체됩니다 

노출 편향이라고 불리는 학습과 추론 사이의 이러한 불일치는 생성된 시퀀스에 따라 빠르게 누적될 수 있는 오류를 야기할 수 있습니다 

단어 수준의 최대 우도 전략의 또 다른 문제점은 학습목표(training objective)가 테스트 측정지표(test metric)과 다르다는 사실입니다

기계번역,대화시스템 등을 평가하는데 쓰이는 측정지표(BLUE, ROUGE)가 단어 수준 최대 우도로 학습된 대화시스템은 둔하고 근시안적인 반응을 생성하는 경향이 있습니다 

그리고 단어 수준 최우도 기반의 텍스트 요약도 역시 비간섭적이거나 반복적인 요약을 생성하는 경향이 있습니다 

강화학습은 위의 문제를 어느 정도 해결할 수 있는 잠재력을 제공한다 
시퀀스 생성작업을 위한 RNN기반의 모델을 학습시키기 위해 강화학습 알고리즘을 적용하는 법과 이전의 지도학습기법과 비교해 새건을 이끌어냈다 이러한 프레임워크에서 시퀀스 생성모델은 환경과 상호작용하는 에이전트로 간주됩니다 

이 에이전트의 파라미터는 정책을 정의한다 정책의 실행으로 에이전트는 각 time step에서 시퀀스의 다음 단어를 예측하는 행동을 취한다 
행동을 취한 다음 에이전트는 내부 상태를 업데이트한다 에이전트가 시퀀스의 끝에 도달하면 보상을 받는다  

생성 문장에 대해 3가지 보상을 정의했다
강화학습 이외의 다른 접근법은 적대적인 학습 기술을 사용하는 것이다 생성기의 학습 목표는 생성된 시퀀스와 진짜 시퀀스를 구별하도록 학습된 다른 판별자를 속이는 것이다 

생성기 G와 판별자 D는 min-max 게임에서 함께 학습되며 생성기는 판별자가 실제 시퀀스와 구별할 수 없는 시퀀스를 생성한다 
이 접근법은 특정 자극을 조건으로 하는 GAN의 변형으로 볼 수 있다 이 같은 프레임워크는 정책 그래디언트를 가진 강화학습 패러다임 하에서도 실현 될 수 있다 

대화시스템의 경우 판별자는 사람과 기계가 생성한 대화를 구분하는 튜링테스트와 유사하다 

