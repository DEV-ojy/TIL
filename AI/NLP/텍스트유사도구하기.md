# 텍스트 유사도 구하기 (주요 알고리즘)

* 자카드 유사도
* 코사인 유사도
* 유클리디언 유사도
* 멘하탄 유사도 

위의 텍스트의 유사도를 구하는 주요 4가지 알고리즘에 대해서 알아보겠습니다

먼저 유사도를 확인할 데이터를 준비합니다 
-텍스트 
1. "휴일인 오늘도 서쪽을 중심으로 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다."

2. "폭염을 피해서 휴일에 놀러왔다가 갑작스런 비로 인해 망연자실하고 있습니다."

사이킷런을 이용하여 텍스트를 수치 벡터화하는 것까지 준비를 해줍니다
이때 TF-IDF벡터라이저를 사용하여줍니다 

```py
from sklearn.feature_extraction.text import TfidfVectorizer
sent = ("휴일인 오늘도 서쪽을 중심으로 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다.", 
    "폭염을 피해서 휴일에 놀러왔다가 갑작스런 비로 인해 망연자실하고 있습니다.")
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(sent) #문장 벡터화 진행
idf = tfidf_vectorizer.idf_
print(dict(zip(tfidf_vectorizer.get_feature_names(), idf)))
```
TF-IDF 벡터화 알고리즘에의해
전체 텍스트에서 모든 단어들을 추출하고 해당 단어들을 발생빈도로 벡터라이징을 해주면 됩니다 

##  [자카드 유사도]

-Jaccard Similarity 또는 자카드 지수는 두 문장을 각각 단어의 집합으로 만든 뒤 두 집합을 통해 유사도를 측정하는 방식중 하나입니다 
-위에서 TF-IDF로 수치벡터화 한 자료는 여기서 쓰이지 않습니다 그냥 두 텍스트의 문자열을 사용합니다 

먼저 방식을 설명하자면 

1. 두 문장에서 단어를 추출하여 각각의 단어 집합으로 만듭니다

2. 두 집합의 교집합인 공통된 단어의 개수를 구합니다

3. 2번에서 구한 개수를 두 집합의 합집합의 개수, 즉 전체 단어수로 나누어줍니다

4. 결과값은 0에서 1 사이로 나올 것입니다

1이라고 한다면, 두 문장에서 완전히 사용된 단어가 동일하다는 것이고, 0이라고 한다면 둘은 교집합이 존재하지 않는 것이므로, 1에 가까운 실수값일수록 서로 유사도가 높다고 표현할수있습니다

- 매우 단순한 방식이니만큼 이외에 별로 말할것도 없습니다
실제 구현의 경우도, 단어 토크나이저 같은 것을 사용하여 단어를 나눈 후,
두 벡터를 비교하며 교집합과 합집합 개수를 구한후 나누어주면 됩니다

## [코사인 유사도]

- Cosine_similarity란, 두 개의 벡터값에서 코사인 각도를 구하는 방식입니다

- 간단히 사용법부터 말하자면
결과값으로 -1에서 1 사이의 값을 가지고, 1에 가까울수록 유사도가 높다는 것으로 해석하고 텍스트 유사도를 구할때 가장 널리 사용되는 방식이자, 성능이 좋다고 합니다

코사인 유사도의 결과값은 '방향'을 나타내며, 좌표점에서의 거리가 아닌, 어디를 가리키는가 하는 각도의 개념을 나타내는 개념이라고 알아둡시다
두 문장 사이에서 각도가 서로 동일한 방향을 가리킨다는 것(1)은, 유사하다는 것이고,서로 반대방향을 가리킨다는 것은(-1) 유사하지 않다는 것입니다

```
- 이에대해 잘 설명된 블로그로는,
https://euriion.com/?p=548를 참고하시길바랍니다
```

- 사이킷런으로 코사인 유사도를 사용하는 방식으로는,
```py
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]) #첫번째와 두번째 문장 비교
#array([[0.113]])
```
이런식으로, tf-idf 벡터에서 문장 두개를 인자값 1, 2에 넣어주면 두 문장의 코사인 유사도가 계산되어 출력됩니다

## [유클리디언 유사도]

-일반적으로`거리`라는 것을 의미하는 유클리디언 좌표공간에서의 두 점의 거리를 나타냅니다 2차원 공간에서 두 점의 구할수 있는 간단한 피타고라스의 정리 공식을 생각하시면 됩니다 

d(x,y) = root((x1-y1)^2 + (x2-y2)^2 + (x3-y3)^2 + ... + (xn-yn)^2)

공식은 위와 같습니다 

이를 Euclidean Distance 혹은 L2-Distance라고 부르며, n차원 공간에서 두 점 사이의 최단거리를 구하는 접근법입니다 

위에서 말했든 중고등학교에 주로 사용하는 좌표평면상 두 점의 거리 계산 방식이 이러한 유클리디언 거리를 뜻하며 좌표평면이라는 것은 특별한게 아니라 그냥 x,y라는 특징을 데이터 차원을 의미하는 공간이므로,유클리디언 공식으로는 몇차원의 거리든 계산이 가능하다는 것이 됩니다 

여기서는 tf-idf로 구한 벡터를 사용하여 거리를 구할 것입니다 
두벡터에서 서로 동일 위치의 값을 빼고, 그것을 제곱하여 더한 후,루트를 씌우면 됩니다 

-사이킷런에서 제공하는 함수로 이를 사용해보자면,

```py
from sklearn.metrics.pairwise import euclidean_distances
euclidean_distances(tfidf_matrix[0:1], tfidf_matrix[1:2])
#array([[1.331]])
```

역시 사이킷런 답게 api 형태가 통일되어있습니다 그런데 유클리디언 유사도의 계산법에 따라서 결과값이 되는 거리는 0에서 무한대로 늘어날수 있습니다 

거리가 가까운 0일수록 유사도가 크고,거리가 크면 그만큼 유사도가 적다는 것입니다

이를 고쳐서 결과값을 0~1 사이의 값으로 정규화 시켜줍니다 

방법은 간단합니다 

최대값이 커지는 이유는 해당 텍스트에서 모든 단어들의 거리의 차이값이 그만큼 커진다는 것이 됩니다 

그렇기에 모든 단어들의 거리의 차이값이 1이되도록 조정해준다면 저절로 결과값이 0~1 사이가 될것입니다 

tf-idf로 구한 단어 빈도의 수치를 모두 더했을 때 1이 되도록 만들어주면 되는 일이니 특징 벡터 내의 모든 값을 더한 값으로 각 원소값들을 나눠주면 됩니다 

이를 L1-Normalization이라고 합니다 

-정규화한 데이터로 다시 유클리디안 거리를 계산하면,
```py
import numpy as np

def l1_normalize(v):
    norm = np.sum(v)
    return v / norm
    
tfidf_norm_l1 = l1_normalize(tfidf_matrix)
euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])
# array([[0.212]])
```

위와 같이 쉽게 데이터 전처리를 해주시면 됩니다 
물론, 이렇게 쉽게 구축이 가능한 것은 모두 넘파이 덕분입니다 
