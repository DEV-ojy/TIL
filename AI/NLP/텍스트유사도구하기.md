# 텍스트 유사도 구하기 (주요 알고리즘)

* 자카드 유사도
* 코사인 유사도
* 유클리디언 유사도
* 멘하탄 유사도 

위의 텍스트의 유사도를 구하는 주요 4가지 알고리즘에 대해서 알아보겠습니다

먼저 유사도를 확인할 데이터를 준비합니다 
-텍스트 
1. "휴일인 오늘도 서쪽을 중심으로 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다."

2. "폭염을 피해서 휴일에 놀러왔다가 갑작스런 비로 인해 망연자실하고 있습니다."

사이킷런을 이용하여 텍스트를 수치 벡터화하는 것까지 준비를 해줍니다
이때 TF-IDF벡터라이저를 사용하여줍니다 

```py
from sklearn.feature_extraction.text import TfidfVectorizer
sent = ("휴일인 오늘도 서쪽을 중심으로 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다.", 
    "폭염을 피해서 휴일에 놀러왔다가 갑작스런 비로 인해 망연자실하고 있습니다.")
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(sent) #문장 벡터화 진행
idf = tfidf_vectorizer.idf_
print(dict(zip(tfidf_vectorizer.get_feature_names(), idf)))
```
TF-IDF 벡터화 알고리즘에의해
전체 텍스트에서 모든 단어들을 추출하고 해당 단어들을 발생빈도로 벡터라이징을 해주면 됩니다 

##  [자카드 유사도]

-Jaccard Similarity 또는 자카드 지수는 두 문장을 각각 단어의 집합으로 만든 뒤 두 집합을 통해 유사도를 측정하는 방식중 하나입니다 
-위에서 TF-IDF로 수치벡터화 한 자료는 여기서 쓰이지 않습니다 그냥 두 텍스트의 문자열을 사용합니다 

먼저 방식을 설명하자면 

1. 두 문장에서 단어를 추출하여 각각의 단어 집합으로 만듭니다

2. 두 집합의 교집합인 공통된 단어의 개수를 구합니다

3. 2번에서 구한 개수를 두 집합의 합집합의 개수, 즉 전체 단어수로 나누어줍니다

4. 결과값은 0에서 1 사이로 나올 것입니다

1이라고 한다면, 두 문장에서 완전히 사용된 단어가 동일하다는 것이고, 0이라고 한다면 둘은 교집합이 존재하지 않는 것이므로, 1에 가까운 실수값일수록 서로 유사도가 높다고 표현할수있습니다

- 매우 단순한 방식이니만큼 이외에 별로 말할것도 없습니다
실제 구현의 경우도, 단어 토크나이저 같은 것을 사용하여 단어를 나눈 후,
두 벡터를 비교하며 교집합과 합집합 개수를 구한후 나누어주면 됩니다

## [코사인 유사도]

- Cosine_similarity란, 두 개의 벡터값에서 코사인 각도를 구하는 방식입니다

- 간단히 사용법부터 말하자면
결과값으로 -1에서 1 사이의 값을 가지고, 1에 가까울수록 유사도가 높다는 것으로 해석하고 텍스트 유사도를 구할때 가장 널리 사용되는 방식이자, 성능이 좋다고 합니다

코사인 유사도의 결과값은 '방향'을 나타내며, 좌표점에서의 거리가 아닌, 어디를 가리키는가 하는 각도의 개념을 나타내는 개념이라고 알아둡시다
두 문장 사이에서 각도가 서로 동일한 방향을 가리킨다는 것(1)은, 유사하다는 것이고,서로 반대방향을 가리킨다는 것은(-1) 유사하지 않다는 것입니다

```
- 이에대해 잘 설명된 블로그로는,
https://euriion.com/?p=548를 참고하시길바랍니다
```

- 사이킷런으로 코사인 유사도를 사용하는 방식으로는,
```py
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]) #첫번째와 두번째 문장 비교
#array([[0.113]])
```
이런식으로, tf-idf 벡터에서 문장 두개를 인자값 1, 2에 넣어주면 두 문장의 코사인 유사도가 계산되어 출력됩니다
