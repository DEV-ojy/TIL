# SVD와 PCA, 그리고 잠재의미분석(LSA)

이번 포스트에서 **차원축소(dimension reduction)**기법으로 널리 쓰이고 있는 **특이값분해(Singular Value Decomposion)**와 **주성분분석(Principal Component Analysis)**에 대해 알아보도록 하겠습니다 마지막으론 이러한 기법이 **잠재의미분석(Latent Sematic Analysis)**와 어떻게 연결되는지도 이야기 해보도록하겠습니다 

## 주성분 분석

PCA는 데이터의 **분산(variance**)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다
이를 그림으로 나타내면 아래와 같습니다. 3차원 공간에 있는 데이터들이 서로 수직인 두 개의 주성분(PC1, PC2)을 새로운 기저로, 선형변환된 것을 확인할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/159473710-37620ae8-f180-4144-901c-2423cd9ed424.png)

원 데이터의 분산을 최대화하는 새로운 기저를 찾는 목표를 달성하려면 우선 데이터 행렬A의 공분산 행렬부터 구해야 합니다 데이터가 각 변수별로 평균이 0으로 맞춰져 있을 때 공분산 행렬은 아래와 같이 구합니다 

![image](https://user-images.githubusercontent.com/80239748/159474046-b861e2ae-7478-440d-9e2e-3de7475dc852.png)

PCA의 새로운 축을 찾기 위해서는 위 공분산행렬을 아래처럼 *고유분해(Eigen decompositoion)*를 수행해주어야 합니다 아래 식에서 A는 대각성분이 공분산행렬의 고유 값이고 나머지 요소는 0인 행렬M,U는 열벡터가 공분산행렬 AA_T의 고유벡터로 이뤄진 행렬입니다 

![image](https://user-images.githubusercontent.com/80239748/159474586-de190fbd-50cc-4de1-9e34-4acfa442a37e.png)

여기에서 A의 대각성분은 데이터행렬 A의 각 변수에 해당하는 분산을 의미합니다 원 데이터의 분산을 최대화하는 새로운 기저를 찾는 것이 목표인 PCA는 

> 가장 큰 고유값 몇 개를 고르고 -> 그에 해당하는 고유벡터를 새로운 기저로 하여 원데이터를 사영

해주면 PCA 작업이 완료되게 됩니다 

예를 들어서 변수가 100개인 데이터에 PCA를 적용한 후 가장 큰 고유값 두 개에 해당하는 고유벡터로 원 데이터를 사영시키면 원데이터의 분산을 최대한 보존하면서도 그 차원수를 100차원 -> 2차원 으로 줄일 수 있게 됩니다 

## 특이값 분해

특이값분해는 mxn 크기의 데이터 행렬 A를 아래와 같이 분해하는 걸 말합니다 

![image](https://user-images.githubusercontent.com/80239748/159661231-433f2dd1-8f9a-48c9-bf0a-87f75a6dc609.png)

행렬 U와V에 속한 열벡터는 **특이벡터(singular vector)**로 불립니다 모든 특이벡터는 서로 직교하는 성질을 지닙니다 

![image](https://user-images.githubusercontent.com/80239748/159661513-0fc4bf12-b9eb-4f89-b0ae-e079dec939e3.png)

행렬 Σ의 특이값은 모두 0보다 크거나 같으며 내림차순으로 정렬돼 있습니다 행렬 Σ의 k번째 대각원소에 해당하는 특이값 Σk는 행렬 AAt의 k번째 고유값에 제곱근을 취한 값과 같습니다 

![image](https://user-images.githubusercontent.com/80239748/159664217-1c66b028-e6d4-403f-aaa5-21e0fee307e9.png)

그러면 특이값 분해를 주성분 분석과 비교해보기 위해 행렬 A를 제곱해 보겠습니다 이후 식을 정리하면 아래와 같습니다 

![image](https://user-images.githubusercontent.com/80239748/159664386-6399e428-9cf1-4927-8ed4-ef9e48ffaa23.png)

여기서 대각성분이 행렬 A의 특이값이고 나머지 성분이 0인 행렬 Σ에 주의할 필요가있습니다 
Σ는 **대가가 행렬(diagonal matrix)**입니다 대각행렬의 거듭제곱은 대각원소들만 거듭제곱을 해준 결과와 같습니다 

따라서  Σ의 제곱은 각 대각원소, 즉 행렬 A의 특이값들을 제곱해준 값과 똑같습니다 그런데 행렬 A의 특이값은 AAt의 고유값에 제곱근을 취한 값과 동일하므로, Σ을 제곱한 행렬은 행렬 AAt의 고유값으로 이뤄진 행렬 A가 됩니다 

이는 정확히 주성분 분석의 결과와 같습니다 

## 특이값 분해의 여러 변형들 

![image](https://user-images.githubusercontent.com/80239748/159909729-2bb3b06d-dd0f-4901-ba1f-4a611b21fdb4.png)

**thin SVD**는 Σ 행렬의 아랫부분과 U에서 여기에 해당하는 부분을 모두 제거합니다 
이렇게 U와 Σ를 줄여도  UsΣsVT로 A를 원복할 수 있습니다 

![image](https://user-images.githubusercontent.com/80239748/160134742-0a11b9ee-56f8-4d27-aad1-e96d4b323317.png)

**compact SVD**는 Σ 행렬에서 비대각파트뿐 아니라 대각원소(특이값)가 0인 부분도 모두 제거한 형태입니다 여기에 대응하는 U와 V의 요소 또한 제거합니다 다시말해 특이값이 양수인 부분만 골라낸다는 뜻입니다 이렇게 U와 Σ,V를 줄여도 UrΣrVTr로 A를 원복할 수 있습니다


![image](https://user-images.githubusercontent.com/80239748/160135102-433ea8b2-9912-45df-a664-928d91ffa961.png)

**truncated SVD**는 Σ 행렬의 대각원소(특이값) 가운데 상위 t개만 골라낸 형태입니다. 이렇게 하면 행렬 A를 원복할 수 없게 되지만, 데이터 정보를 상당히 압축했음에도 행렬 A를 근사할 수 있게 됩니다. 이후 설명드릴 잠재의미분석은 바로 이 방법을 사용합니다

![image](https://user-images.githubusercontent.com/80239748/160135173-e79dc692-bdcd-4f7e-b006-984a142d0050.png)

## 잠재의미분석 개요

다음과 같은 문서들이 있다고 하면, 우리는 이를 토대로 단어-문서행렬 A를 만들 수 있습니다

> doc1 : 나,는,학교,에,가,ㄴ,다
> doc2 : 학교,에,가,는,영희
> doc3 : 나,는,영희,는,좋,다

|-|doc1|doc2|doc3|
|-----|-------|------|------|
|나|1|0|0|
|는|1|1|2|
|학교|1|1|0|
|에|1|1|0|
|가|1|1|0|
|ㄴ|1|0|0|
|다|1|0|1|
|영희|0|1|1|
|좋|0|0|1|

*잠재의미분석*이란 위와 같은 **단어-문서행렬(Word-Document Matrix),단어-문맥행렬(window based co-occurrence matrix)** 등 입력 데이터에 특이값 분해를 수행해 데이터의 차원수를 줄여 계산 효율성을 키우는 한편 행간에 숨어있는(latent) 의미를 이끌어내기 위한 방법론 입니다 

대략적인 개념은 아래 그림과 같습니다 

![image](https://user-images.githubusercontent.com/80239748/160136299-7350b642-1c64-41fc-9c77-e6ab0621f021.png)

잠재의미분석을 수행하는 절차는 이렇습니다 n개의 문서를 m개의 단어로 표현된 입력데이터 행렬 A가 주어졌다고 칩시다 A의 0보다 큰 고유값의 개수를 r이라고 할 때, r보다 작은 k를 연구자가 임의로 설정하고 Σk를 만듭니다 

이후 U와 V 행렬에서 여기에 대응하는 부분만 남겨 Uk와 Vk를 만들어줍니다. 이렇게 되면 A와 비슷한 Ak 행렬을 구축할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/160242385-5deade97-b439-4aa4-acfe-aace6f5b206f.png)

위 식 양변에 Uk의 전치행렬을 곱해준 것을 X1, Vk를 곱해준 것을 X2라고 둡니다 그러면 X1의 경우 n개의 문서는 원래 단어수 m보다 훨씬 작은 k개 변수로 표현된 결과가 됩니다 
X2 는 m개의 단어가 원래 문서 수 n보다 작은 k개 변수로 표현한 결과입니다

이는 주성분 분석에서의 차원축소 효과와 비슷한 것으로 이해하면 좋을 것 같습니다

![image](https://user-images.githubusercontent.com/80239748/160242397-a696c838-66ef-4933-8b94-fe2d0e65b718.png)

## 잠재의미분석 예시

자, 그럼 위에서 예로 든 단어-문서행렬 A를 가지고 잠재의미분석을 수행해 보겠습니다
A에 SVD를 수행하면 아래와 같이 쓸 수 있습니다. 숫자들이 엄청 많은데요, ‘아 이렇게 분해될 수 있구나’ 정도로 보고 슥 넘어가시면 될 것 같습니다

![image](https://user-images.githubusercontent.com/80239748/160277928-ca20a186-8347-4e18-bb6e-42ccdaeb70c4.png)

그럼 Σ 행렬의 특이값 가운데 상위 2개(3.61과 2.04)만 남기고 나머지를 제거하는 방식으로 truncated SVD를 수행해 보겠습니다

아래와 같습니다 

![image](https://user-images.githubusercontent.com/80239748/160277945-3c2defcf-d271-4508-985f-3cc79ba69ee7.png)

A′ 의 각 요소값을 반올림해서 원 행렬 A와 비교해보겠습니다 6행과 7행의 요소값이 조금 다를 뿐 거의 유사한 것을 확인할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/160277955-2fe149fa-78a0-4f47-a933-39d47fe8db84.png)

그럼 A′=U2Σ2VT2 양변의 맨 앞에 U2의 전치행렬을 각각 곱해줄까요? 그 결과를 X1이라고 두면 아래와 같습니다

![image](https://user-images.githubusercontent.com/80239748/160277968-68258a74-d241-4e4e-9577-4196d366d95a.png)

원데이터인 A와 SVD 결과인 X1의 열(column)은 문서(문장)를 의미합니다 단어-문맥행렬 A에서 각 문서들은 9개 단어(변수)들로 표현됐으나 X1에서는 단 두 개의 변수들만으로도 표현이 가능해졌습니다

이렇게 만든 X1에 다양한 데이터마이닝 기법을 적용해 여러 가지 문제를 풀게 되는 것입니다