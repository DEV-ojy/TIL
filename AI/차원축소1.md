# SVD와 PCA, 그리고 잠재의미분석(LSA)

이번 포스트에서 **차원축소(dimension reduction)**기법으로 널리 쓰이고 있는 **특이값분해(Singular Value Decomposion)**와 **주성분분석(Principal Component Analysis)**에 대해 알아보도록 하겠습니다 마지막으론 이러한 기법이 **잠재의미분석(Latent Sematic Analysis)**와 어떻게 연결되는지도 이야기 해보도록하겠습니다 

## 주성분 분석

PCA는 데이터의 **분산(variance**)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법입니다
이를 그림으로 나타내면 아래와 같습니다. 3차원 공간에 있는 데이터들이 서로 수직인 두 개의 주성분(PC1, PC2)을 새로운 기저로, 선형변환된 것을 확인할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/159473710-37620ae8-f180-4144-901c-2423cd9ed424.png)

원 데이터의 분산을 최대화하는 새로운 기저를 찾는 목표를 달성하려면 우선 데이터 행렬A의 공분산 행렬부터 구해야 합니다 데이터가 각 변수별로 평균이 0으로 맞춰져 있을 때 공분산 행렬은 아래와 같이 구합니다 

![image](https://user-images.githubusercontent.com/80239748/159474046-b861e2ae-7478-440d-9e2e-3de7475dc852.png)

PCA의 새로운 축을 찾기 위해서는 위 공분산행렬을 아래처럼 *고유분해(Eigen decompositoion)*를 수행해주어야 합니다 아래 식에서 A는 대각성분이 공분산행렬의 고유 값이고 나머지 요소는 0인 행렬M,U는 열벡터가 공분산행렬 AA_T의 고유벡터로 이뤄진 행렬입니다 

![image](https://user-images.githubusercontent.com/80239748/159474586-de190fbd-50cc-4de1-9e34-4acfa442a37e.png)

여기에서 A의 대각성분은 데이터행렬 A의 각 변수에 해당하는 분산을 의미합니다 원 데이터의 분산을 최대화하는 새로운 기저를 찾는 것이 목표인 PCA는 

> 가장 큰 고유값 몇 개를 고르고 -> 그에 해당하는 고유벡터를 새로운 기저로 하여 원데이터를 사영

해주면 PCA 작업이 완료되게 됩니다 

예를 들어서 변수가 100개인 데이터에 PCA를 적용한 후 가장 큰 고유값 두 개에 해당하는 고유벡터로 원 데이터를 사영시키면 원데이터의 분산을 최대한 보존하면서도 그 차원수를 100차원 -> 2차원 으로 줄일 수 있게 됩니다 

## 특이값 분해

특이값분해는 mxn 크기의 데이터 행렬 A를 아래와 같이 분해하는 걸 말합니다 

![image](https://user-images.githubusercontent.com/80239748/159661231-433f2dd1-8f9a-48c9-bf0a-87f75a6dc609.png)

행렬 U와V에 속한 열벡터는 **특이벡터(singular vector)**로 불립니다 모든 특이벡터는 서로 직교하는 성질을 지닙니다 

![image](https://user-images.githubusercontent.com/80239748/159661513-0fc4bf12-b9eb-4f89-b0ae-e079dec939e3.png)

행렬 Σ의 특이값은 모두 0보다 크거나 같으며 내림차순으로 정렬돼 있습니다 행렬 Σ의 k번째 대각원소에 해당하는 특이값 Σk는 행렬 AAt의 k번째 고유값에 제곱근을 취한 값과 같습니다 

![image](https://user-images.githubusercontent.com/80239748/159664217-1c66b028-e6d4-403f-aaa5-21e0fee307e9.png)

그러면 특이값 분해를 주성분 분석과 비교해보기 위해 행렬 A를 제곱해 보겠습니다 이후 식을 정리하면 아래와 같습니다 

![image](https://user-images.githubusercontent.com/80239748/159664386-6399e428-9cf1-4927-8ed4-ef9e48ffaa23.png)

여기서 대각성분이 행렬 A의 특이값이고 나머지 성분이 0인 행렬 Σ에 주의할 필요가있습니다 
Σ는 **대가가 행렬(diagonal matrix)**입니다 대각행렬의 거듭제곱은 대각원소들만 거듭제곱을 해준 결과와 같습니다 

따라서  Σ의 제곱은 각 대각원소, 즉 행렬 A의 특이값들을 제곱해준 값과 똑같습니다 그런데 행렬 A의 특이값은 AAt의 고유값에 제곱근을 취한 값과 동일하므로, Σ을 제곱한 행렬은 행렬 AAt의 고유값으로 이뤄진 행렬 A가 됩니다 

이는 정확히 주성분 분석의 결과와 같습니다 

## 특이값 분해의 여러 변형들 

![image](https://user-images.githubusercontent.com/80239748/159909729-2bb3b06d-dd0f-4901-ba1f-4a611b21fdb4.png)

**thin SVD**는 Σ 행렬의 아랫부분과 U에서 여기에 해당하는 부분을 모두 제거합니다 
이렇게 U와 Σ를 줄여도  UsΣsVT로 A를 원복할 수 있습니다 

![image](https://user-images.githubusercontent.com/80239748/160134742-0a11b9ee-56f8-4d27-aad1-e96d4b323317.png)

**compact SVD**는 Σ 행렬에서 비대각파트뿐 아니라 대각원소(특이값)가 0인 부분도 모두 제거한 형태입니다 여기에 대응하는 U와 V의 요소 또한 제거합니다 다시말해 특이값이 양수인 부분만 골라낸다는 뜻입니다 이렇게 U와 Σ,V를 줄여도 UrΣrVTr로 A를 원복할 수 있습니다


![image](https://user-images.githubusercontent.com/80239748/160135102-433ea8b2-9912-45df-a664-928d91ffa961.png)

**truncated SVD**는 Σ 행렬의 대각원소(특이값) 가운데 상위 t개만 골라낸 형태입니다. 이렇게 하면 행렬 A를 원복할 수 없게 되지만, 데이터 정보를 상당히 압축했음에도 행렬 A를 근사할 수 있게 됩니다. 이후 설명드릴 잠재의미분석은 바로 이 방법을 사용합니다

![image](https://user-images.githubusercontent.com/80239748/160135173-e79dc692-bdcd-4f7e-b006-984a142d0050.png)