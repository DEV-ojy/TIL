
# RNN(Recurrent Neual Network) - 순환신경망  

아래에 네 개의 단어가 있습니다 
## I work at google

`I`는 주어, `work`가 동사, `at` 은 전치사, `google`은 명사라는 것은 대부분 알고 있습니다 
좀 더 구체적으로 들여다보면 주어인 I가 왔기때문에 그 뒤는 동사라는 것이 자연스레 예측됐고
전치사인 at이 왔기 떄문에 그 뒤는 명사가 올것이라고 추론할수있었습니다 
**이러한 추론과정을 수학적으로 모델링한 것이 바로 RNN 입니다 **  

## RNN

인공지능을 개발하는데 있어서 인공 신경망의 한 종류이며 딥러닝에 있어 가장 기본적인 시퀀스 모델입니다

하나의 단어만 이해 한다 해서 전체의 맥락을 이해하는 것이 아닌 그 전에 이야기 했던 단어를 이해한 다음에 어떤단어를 말할때 다 이해가 되는 것을 말합니다 

![Sequence](https://user-images.githubusercontent.com/80239748/121489612-3821ee00-ca0f-11eb-9be7-c3c42795ed25.JPG)

이걸 풀어서 보면

![Sequence2](https://user-images.githubusercontent.com/80239748/121496553-8a660d80-ca15-11eb-857f-8bfbcb3c294d.JPG)

그림에서 보시다 시피 내부의 순환 구조가 포함되어 있기 때문에 어떤 시점을 알기위해 이전의 연산들이 영향을 주는 시리즈의 데이터 
**즉, 시간의 의존적이거나 순차적인 데이터 학습에 활용될수있습니다.**

RNN이 기존 뉴럴 네트워크와 다른 점은 **기억(hidden state)** 를 가지고 있다 네트워크의 기억은 지금까지의 입력 데이터를 요약한 정보라 할 수 있습니다 

새로운 입력이 들어올때마다 네트워크는 자신의 기억을 조금씩 수정해 나가면서
**결국,입력을 다 처리 하고 나면 남겨진 기억은 시퀀스 전체를 요약하는 정보가 됩니다.** 

내부에 있는 순환 구조에 의해 현재 정보에 이전 정보가 쌓이면서 정보 표현이 가능한 알고지름으로, 데이터가 순환되기 때문에 정보가 끊임없이 갱신될 수 있는 구조입니다.

이러한 점 때문에 여러 단계에서 매개 변수 공유 → 훈련 매개 변수 감소 및 계산 비용 감소 반복적이고 순차적인 데이터에 효과 나타낼수 있습니다 

## RNN구조 예시

위에서 보았던 `I work at google`를 input으로 받아 품사가 무엇인지 output으로 알려주는 RNN으로  예시를 들어보겠습니다 이때,input은 동사가 아닌 순차적으로 들어오며, `I`는 hidden state라는 것을 거쳐 **주어**라는 것을 미리 알고 있다고 가정합니다

![image](https://user-images.githubusercontent.com/80239748/135945449-c9117da9-3bcc-44c3-9557-cd7bf1beab52.png)

그러면 `I` 다음 `work`를 input으로 받는데 이때, hidden state는 `work`뿐만 아니라 이전 `I`에대한 데이터를 이전 hidden state를 통해 받아 **동사**라고 판단하게 됩니다 

마지막으로 `google`이 들어왔을때는 `google`과 함께  'I','work','at'에 대한 정보를 함께 조합하여 `google`이 명사일 확률이 높다고 결론을 내리게 됩니다 

## 그렇다면 RNN으로 무엇을 할 수 있을까?

![models](https://user-images.githubusercontent.com/80239748/121499962-d36b9100-ca18-11eb-99b7-6b268d97e421.JPG)

### 1. 고정크기 입력 & 고정크기 출력 
    순환적인 부분이 없기 때문에 RNN이 아니다

### 2. 고정크기 입력 & 시퀀스 출력 
    이미지 캡션 생성
    이미지를 입력해서 이미지에 대한 설명을 문자으로 출력하는 이미지 캡션을 생성

### 3. 시퀀스 입력 & 고정크기 출력 
    문장을 입력해서 긍부정 정도를 출력하는 감성 분석기 

### 4. 시퀀스 입력 & 시퀀스 출력 
    자동번역 
    구글의 번역기와 네이버의 파파고는 RNN을 응용한 모델로 만들어졌다
    RNN기반 모델은 기존 통계 기반 모델의 비해 우수한 성능을 낸다
    이러한 모델을 다른 말로 encoder-decoder 모델이라고 한다

### 5. 동기화된 시퀀스 입력 & 시퀀스 출력
    문장에서 다음에 나올 단어를 예츨하는 언어모델 
    
# RNN의 단점 

RNN은 문장에서는 필요한 정보를 얻기 위한 시간 격차가 크지 않으면지난 정보를 바탕으로 학습을 할 수 있습니다

![image](https://user-images.githubusercontent.com/80239748/122632482-0515e380-d10e-11eb-85c6-fcc22a9491fb.png)

하지만 반대로 더 많은 문맥을 필요로하는 경우 필요한 정보를 얻기 위한 시간 격차가 커지면 RNN은 학습하는 정보를 계속 이어나가기 힘들어집니다

![image](https://user-images.githubusercontent.com/80239748/122632497-1828b380-d10e-11eb-9b03-e2b4b4312005.png)

이론적으론 RNN이 이러한 **긴 기간의 의존성**을 완벽하게 다룰 수 있다고는 하지만 실제 RNN은 문제를 해결하지 못 하는 것이 슬픈 현실입니다  

하지만 이러한 문제점을 보안하는 RNN의 특별한 한 종류 **LSTM (Long Short-Term Memory) - 장단기 메모리가 있습니다**

다음에는 RNN의 단점을 보안한 LSTM-장단기메모리에 대해 알아보겠습니다 

#### RNN참고 문서 https://box-world.tistory.com/39